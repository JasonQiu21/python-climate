{
 "cells": [
  {
   "source": [
    "This notebook contains code for calculating the tas anomaly of a given dataset. We use the Pangeo CMIP6 Public Dataset. A lot of this code was modified starting from code from the Pangeo Gallery,found [here](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/).\n",
    "\n",
    "Additionally, the [xarray documentation](https://xarray.pydata.org/en/stable/) and [xarray-extras documentation](https://xarray-extras.readthedocs.io/en/latest/), along with the online documentation for any of the other used libraries are a useful resource."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm\n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "import cftime\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")"
   ]
  },
  {
   "source": [
    "After importing all relevant packages and using intake to open the pangeo cmip6 database, we construct our query. Note that this particular query requires all sources have all 5 experiments. This is due to the passing the .search() function the perameter `require_all_on`. Info at the [intake-esm docs](https://intake-esm.readthedocs.io/en/latest/api.html?highlight=search#intake_esm.core.esm_datastore.search)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ed47778ad250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmember_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r1i1p1f1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequire_all_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# In the other two documents, I used col_subset instead of cat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "expts = ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "query = dict(\n",
    "    experiment_id = expts,\n",
    "    variable_id = ['tas'],\n",
    "    table_id = ['Amon'],\n",
    "    source_id = ['ACCESS-ESM1-5','CESM2-WACCM', 'MPI-ESM1-2-HR'],\n",
    "    member_id = 'r1i1p1f1'\n",
    ")\n",
    "\n",
    "# In the other two documents, I used col_subset instead of cat.\n",
    "cat = col.search(require_all_on=['source_id'], **query)\n",
    "\n",
    "\n",
    "print(col_subset.df.groupby(\"source_id\")[\n",
    "    [\"experiment_id\", \"variable_id\", \"table_id\"]\n",
    "].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an xarray dataset using our catalog.\n",
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': False},cdf_  kwargs={'chunks': {}, 'decode_times': False})"
   ]
  },
  {
   "source": [
    "The next block of code is mainly for formatting all of the datasets into a more standard format. This is especially important if you want to make a graph using multiple"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_bounds(ds):\n",
    "    \"\"\"Drop coordinates like 'time_bounds' from datasets,\n",
    "    which can lead to issues when merging.\"\"\"\n",
    "    drop_vars = [vname for vname in ds.coords if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "def open_dsets(df):\n",
    "    \"\"\"Open datasets from cloud storage and return xarray dataset.\"\"\"\n",
    "    dsets = [xr.open_zarr(fsspec.get_mapper(ds_url), consolidated=True).pipe(drop_all_bounds) for ds_url in df.zstore]\n",
    "    try:\n",
    "        ds = xr.merge(dsets, join='exact')\n",
    "        return ds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def open_delayed(df):\n",
    "    \"\"\"A dask.delayed wrapper around `open_dsets`.\n",
    "    Allows us to open many datasets in parallel.\"\"\"\n",
    "    return dask.delayed(open_dsets)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dsets = defaultdict(dict)\n",
    "for group, df in cat.df.groupby(by=['source_id', 'experiment_id']):\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)\n",
    "\n",
    "open_dsets(df)\n",
    "dsets_ = dask.compute(dict(dsets))[0]\n",
    "\n",
    "expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id', coords={'experiment_id': expts})\n",
    "\n",
    "dsets_aligned = {}"
   ]
  },
  {
   "source": [
    "The main dictionary we will be working with is `dsets_`. Generally, `dsets_[source][scenario]` will be an xarray dataset with its data stored as a dask array.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dsets_"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'ACCESS-ESM1-5': {'historical': <xarray.Dataset>\n  Dimensions:  (lat: 145, lon: 192, time: 1980)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -90.0 -88.75 -87.5 -86.25 ... 86.25 87.5 88.75 90.0\n    * lon      (lon) float64 0.0 1.875 3.75 5.625 7.5 ... 352.5 354.4 356.2 358.1\n    * time     (time) datetime64[ns] 1850-01-16T12:00:00 ... 2014-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(589, 145, 192), meta=np.ndarray>,\n  'ssp126': <xarray.Dataset>\n  Dimensions:  (lat: 145, lon: 192, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -90.0 -88.75 -87.5 -86.25 ... 86.25 87.5 88.75 90.0\n    * lon      (lon) float64 0.0 1.875 3.75 5.625 7.5 ... 352.5 354.4 356.2 358.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(516, 145, 192), meta=np.ndarray>,\n  'ssp245': <xarray.Dataset>\n  Dimensions:  (lat: 145, lon: 192, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -90.0 -88.75 -87.5 -86.25 ... 86.25 87.5 88.75 90.0\n    * lon      (lon) float64 0.0 1.875 3.75 5.625 7.5 ... 352.5 354.4 356.2 358.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(516, 145, 192), meta=np.ndarray>,\n  'ssp370': <xarray.Dataset>\n  Dimensions:  (lat: 145, lon: 192, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -90.0 -88.75 -87.5 -86.25 ... 86.25 87.5 88.75 90.0\n    * lon      (lon) float64 0.0 1.875 3.75 5.625 7.5 ... 352.5 354.4 356.2 358.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(516, 145, 192), meta=np.ndarray>,\n  'ssp585': <xarray.Dataset>\n  Dimensions:  (lat: 145, lon: 192, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -90.0 -88.75 -87.5 -86.25 ... 86.25 87.5 88.75 90.0\n    * lon      (lon) float64 0.0 1.875 3.75 5.625 7.5 ... 352.5 354.4 356.2 358.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(590, 145, 192), meta=np.ndarray>},\n 'CESM2-WACCM': {'historical': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 288, time: 1980)\n  Coordinates:\n    * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n    * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n    * time     (time) object 1850-01-15 12:00:00 ... 2014-12-15 12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(600, 192, 288), meta=np.ndarray>,\n  'ssp126': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 288, time: 1032)\n  Coordinates:\n    * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n    * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n    * time     (time) object 2015-01-15 12:00:00 ... 2100-12-15 12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(816, 192, 288), meta=np.ndarray>,\n  'ssp245': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 288, time: 1032)\n  Coordinates:\n    * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n    * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n    * time     (time) object 2015-01-15 12:00:00 ... 2100-12-15 12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(600, 192, 288), meta=np.ndarray>,\n  'ssp370': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 288, time: 1032)\n  Coordinates:\n    * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n    * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n    * time     (time) object 2015-01-15 12:00:00 ... 2100-12-15 12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(816, 192, 288), meta=np.ndarray>,\n  'ssp585': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 288, time: 1032)\n  Coordinates:\n    * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n    * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n    * time     (time) object 2015-01-15 12:00:00 ... 2100-12-15 12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(600, 192, 288), meta=np.ndarray>},\n 'MPI-ESM1-2-HR': {'historical': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 384, time: 1980)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -89.28 -88.36 -87.42 -86.49 ... 87.42 88.36 89.28\n    * lon      (lon) float64 0.0 0.9375 1.875 2.812 ... 356.2 357.2 358.1 359.1\n    * time     (time) datetime64[ns] 1915-01-16T12:00:00 ... 1959-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(422, 192, 384), meta=np.ndarray>,\n  'ssp126': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 384, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -89.28 -88.36 -87.42 -86.49 ... 87.42 88.36 89.28\n    * lon      (lon) float64 0.0 0.9375 1.875 2.812 ... 356.2 357.2 358.1 359.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(845, 192, 384), meta=np.ndarray>,\n  'ssp245': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 384, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -89.28 -88.36 -87.42 -86.49 ... 87.42 88.36 89.28\n    * lon      (lon) float64 0.0 0.9375 1.875 2.812 ... 356.2 357.2 358.1 359.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(600, 192, 384), meta=np.ndarray>,\n  'ssp370': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 384, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -89.28 -88.36 -87.42 -86.49 ... 87.42 88.36 89.28\n    * lon      (lon) float64 0.0 0.9375 1.875 2.812 ... 356.2 357.2 358.1 359.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(845, 192, 384), meta=np.ndarray>,\n  'ssp585': <xarray.Dataset>\n  Dimensions:  (lat: 192, lon: 384, time: 1032)\n  Coordinates:\n      height   float64 ...\n    * lat      (lat) float64 -89.28 -88.36 -87.42 -86.49 ... 87.42 88.36 89.28\n    * lon      (lon) float64 0.0 0.9375 1.875 2.812 ... 356.2 357.2 358.1 359.1\n    * time     (time) datetime64[ns] 2015-01-16T12:00:00 ... 2100-12-16T12:00:00\n  Data variables:\n      tas      (time, lat, lon) float32 dask.array<chunksize=(600, 192, 384), meta=np.ndarray>}}"
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "source": [
    "A lot of code I used ended up needing the line `climatology = v['historical'].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.month').mean('time')` or something similar. The `sel`, `groupby`, and `mean` methods are all in xarray's documetation (linked at the start of this notebook). For `slice`, see [here](https://docs.python.org/3/library/functions.html#slice)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tqdm(dsets_.items()):\n",
    "\n",
    "    # Check for any missing experiments that we expect\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    climatology = v['historical'].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.month').mean('time')\n",
    "\n",
    "    for i in v:\n",
    "        if i == 'historical':\n",
    "            # When working with daily data, it's very easy for objects to run your computer out of memory, so we shorten historical runs because to the date range we need.\n",
    "            anomaly = v[i].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.month') - climatology\n",
    "        else:\n",
    "            anomaly = v[i].groupby('time.month') - climatology\n",
    "        # Because these files are too large to store in memory, we use the option compute=False to create a dask delayed object and then compute it later.\n",
    "        # Remember to change the file location to the relevant folder.\n",
    "        delayed_obj = anomaly.to_netcdf(path=f\"~/tas-anomaly_{k}_{i}.nc\", mode='w', compute=False, engine='netcdf4', format='NETCDF4')\n",
    "        print(f\"writing data to ~/tas-anomaly_{k}_{i}.nc\")\n",
    "\n",
    "        with progress.ProgressBar():\n",
    "            results = delayed_obj.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}